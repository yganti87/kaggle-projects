# Wine Quality Prediction with Linear Regression

This notebook demonstrates how to predict wine quality scores using a Linear Regression model from scikit-learn.

## ğŸ· Prediction Problem

### **What are we trying to predict?**
We're predicting the **quality score** of red wines based on their chemical properties. The quality score ranges from 3 to 8, where higher scores indicate better quality wines.

### **Input Features (Chemical Properties):**
- **Fixed acidity**: Tartaric acid content (g/dmÂ³)
- **Volatile acidity**: Acetic acid content (g/dmÂ³) - too much can make wine taste vinegary
- **Citric acid**: Citric acid content (g/dmÂ³) - adds freshness and flavor
- **Residual sugar**: Sugar remaining after fermentation (g/dmÂ³)
- **Chlorides**: Salt content (g/dmÂ³)
- **Free sulfur dioxide**: Free SOâ‚‚ content (mg/dmÂ³) - prevents microbial growth
- **Total sulfur dioxide**: Total SOâ‚‚ content (mg/dmÂ³)
- **Density**: Wine density (g/cmÂ³)
- **pH**: Acidity level (0-14 scale)
- **Sulphates**: Potassium sulphate content (g/dmÂ³)
- **Alcohol**: Alcohol content (% by volume)

### **Target Variable:**
- **Quality**: Wine quality score (3-8 scale, integer values)

## ğŸ“Š Dataset Information

- **Source**: UCI Machine Learning Repository
- **Dataset**: Wine Quality (Red Wine)
- **Size**: ~1,600 samples
- **Features**: 11 chemical properties
- **Target**: Quality score (3-8)

## ğŸ¤– Linear Regression Model

### **What is Linear Regression?**
Linear Regression is a supervised learning algorithm that models the relationship between a dependent variable (target) and one or more independent variables (features) using a linear function.

### **Mathematical Formula:**
```
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ
```
Where:
- `y` = predicted quality score
- `Î²â‚€` = intercept (baseline quality)
- `Î²áµ¢` = coefficient for feature i
- `xáµ¢` = value of feature i
- `Îµ` = error term (residual)

### **scikit-learn's LinearRegression Implementation:**

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
```

**Key Features:**
- **Ordinary Least Squares (OLS)**: Minimizes the sum of squared residuals
- **No regularization**: Pure linear regression without penalty terms
- **Fast training**: O(nÂ²p) complexity where n=samples, p=features
- **Interpretable**: Coefficients show feature importance and direction

**Model Parameters:**
- `fit_intercept=True`: Whether to calculate the intercept (Î²â‚€)
- `copy_X=True`: Whether to copy X or overwrite it
- `n_jobs=None`: Number of jobs for parallel computation

**Model Attributes:**
- `coef_`: Feature coefficients (Î²â‚, Î²â‚‚, ..., Î²â‚™)
- `intercept_`: Intercept term (Î²â‚€)

## ğŸ“ˆ Model Evaluation Methods

### **1. Quantitative Metrics:**

#### **Mean Squared Error (MSE):**
```python
mse = mean_squared_error(y_test, y_pred)
```
- Measures average squared difference between predictions and actual values
- Lower values = better performance
- Units: squared quality points
- **Interpretation**: Average squared prediction error

#### **RÂ² Score (Coefficient of Determination):**
```python
r2 = r2_score(y_test, y_pred)
```
- Measures proportion of variance explained by the model
- Range: 0 to 1 (1 = perfect predictions)
- **Interpretation**: 
  - RÂ² = 0.7 means model explains 70% of variance
  - RÂ² = 0.3 means model explains 30% of variance

### **2. Visual Evaluation Methods:**

#### **A. Actual vs Predicted Scatter Plot:**
- **What to look for**: Points close to diagonal line (y=x)
- **Good fit**: Points clustered around diagonal
- **Poor fit**: Points scattered far from diagonal

#### **B. Residual Plot:**
- **What to look for**: Random scatter around y=0 line
- **Good fit**: No patterns, random distribution
- **Problems**: Curves, funnels, or clusters indicate model issues

#### **C. Residual Distribution (Histogram):**
- **What to look for**: Normal distribution shape
- **Good fit**: Bell-shaped curve centered at zero
- **Problems**: Skewed or non-normal distribution

#### **D. Q-Q Plot:**
- **What to look for**: Points on straight line
- **Good fit**: Residuals follow normal distribution
- **Problems**: Points deviate from line

#### **E. Feature Importance Plot:**
- **What to look for**: Which features have strongest influence
- **Interpretation**: Larger absolute coefficients = more important features

## ğŸ¯ Model Assumptions

Linear Regression assumes:
1. **Linearity**: Relationship between features and target is linear
2. **Independence**: Residuals are independent of each other
3. **Homoscedasticity**: Residuals have constant variance
4. **Normality**: Residuals follow normal distribution

## ğŸ” What Makes a "Good" Linear Regression Model?

### **Good Model Indicators:**
- âœ… **High RÂ² score** (close to 1.0)
- âœ… **Low MSE** (close to 0)
- âœ… **Random residuals** (no patterns in residual plot)
- âœ… **Normal residual distribution**
- âœ… **Points close to diagonal** in actual vs predicted plot

### **Problem Indicators:**
- âŒ **Low RÂ² score** (< 0.3)
- âŒ **High MSE** (large prediction errors)
- âŒ **Patterned residuals** (curves, funnels, clusters)
- âŒ **Non-normal residual distribution**
- âŒ **Points far from diagonal** in actual vs predicted plot

## ğŸš€ Running the Notebook

1. **Open** `wine_linear_regression.ipynb` in JupyterLab
2. **Run cells sequentially** to:
   - Load and explore the dataset
   - Visualize data distributions and correlations
   - Train the Linear Regression model
   - Evaluate performance with comprehensive visualizations
3. **Interpret results** using the evaluation methods above

## ğŸ“š Key Takeaways

- **Linear Regression** is a simple but powerful baseline model
- **Visual evaluation** is crucial for understanding model performance
- **Residual analysis** reveals model assumptions and potential issues
- **Feature importance** helps understand what drives wine quality
- **Multiple evaluation methods** provide comprehensive model assessment

This notebook serves as a foundation for understanding both the wine quality prediction problem and linear regression model evaluation techniques. 